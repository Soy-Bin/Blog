# Boosting
- 이전 학습을 보완하는 방식으로 학습되는 앙상블 기법
- 트리가 앙상블했을 때 효과가 좋다.
- Bagging과의 비교
	- 
## Ada Boost(1990년 말~)
 ![](https://i.imgur.com/3gz67Kx.png)
	[Image Link](https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/)
- 학습하고 있는 모델을 더 잘 학습하는 것이 목표 cf) bagging : variance를 늘려서 과적합을 방지하는 것이 목표
- 원리
	1. misclassify되거나 loss가 큰 데이터에 확률 가중치를 부여하여 다음 학습에서 sampling할 때 더 많이 뽑히도록 하여 학습한다.
	2. 1을 반복한다.
	3. 여러 모델에 학습 데이터를 inference한 결과를 취합한다.(tree : 최빈값)
## Gradient Boosting Machine
- Boosting+Gradient Descent Algorithm : error 자체를 계산하여 에러를 줄이는 방향으로 다음 모델을 만들어 개선하는 방법
- Boosting에서 가중치 부여 후 sampling하는 것이 성능 개선에 직접적 영향은 없다는 한계 때문에 나옴
- regression 모델에 주로 적용 (loss를 넘겨줘서 계산한 결과 함수를 최종적으로 더해야 하기 때문에 분류에는 적용이 어려움)
![](https://i.imgur.com/tW7i6Qr.png)
[Image Link](https://esj205.oopy.io/98c51457-8a92-455c-861c-0f35667daf33)
- 다음 모델의 target value $y$는 앞 전 모델의 오차이며, 오차가 0에 가까워 지는 것이 목표
	- $y \sim f_1(x)+f_2(x)+\cdots +f_n(x)$
	- pseudo-residual : $r_{i,j}=\alpha r_{i-1,j}-f_i(x_j)$ 
		- $r_{i,j}$ : i번째 모델의 j번째 데이터에 대한 오차. 보정값 $\alpha$가 붙었으므로 가짜 오차.
		- $f_i$ : 앞 전 모델의 error를 근사하는 함수
		- $\alpha$ : learning rate(overfitting되지 않도록 regularization. 이 모델은 매우 overfitting이 잘 됨)
## XGBoost
- 잘 만들어진 open source library(C++ 기반) -> 대중화 됨
- Gradient Boosting Decision Tree 기반
- 알고리즘 개선은 없으나 하드웨어 최적화로 속도를 엄청나게 향상 시킴(Parallelized tree building, Cache awareness and out-of-core computing) -> 머신러닝 라이브러리 중 인기있는 라이브러리
![](https://i.imgur.com/uY7l21w.png)
[Image Link](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)
- 결측치 처리 기능이 있으나 권장하지 않음
- Hyper-parameter Tuning(scikit-learn API) : $1000\leq N\leq 30000$ 권장(코랩 기준)
	- CART관련
		- max_depth : (**5~14**)
		- colsample_bynode : local randomization(**0.5~0.8**) -> 너무 작은 수치를 쓰면 관련 없는 게 뽑혔을 때의 결과와 잘 뽑혔을 때 결과의 차이가 너무 커짐
		- reg_lambda : regularization parameter. 알파가 아님. (**0.5~5**)
	- Tree ensmble 관련
		- n_estimators : tree 수(**50~2000**)
		- learning_rate : pseudo-residual parameter(**0.3~0.01**) -> N이 커질수록 이 값이 작아져야 overfitting 안 됨
		- tree_method : best split을 찾을 때 데이터를 사용하는 방법
			- 'exact' : 전부 다. 성능은 이게 젤 좋지만 느림
			- 'approximate' : 일종의 cpu-hist. 성능이 별로라서 잘 안 씀
			- 'gpu-hist' : 데이터가 많을 때
			- 'auto' : default
		- random_state
		- early_stopping_rounds : 지정된 만큼의 tree를 만드는 동안 성능 개선이 안 되면 멈춤 (eval_metric,validaton_data를 함께 설정해야 사용 가능) -> overfitting 방지에 도
## LightGBM(2017)
- XGBoost보다 가볍고 빠른 GBDT모델(Microsoft에서 만듦)
- leaf-wise growth : leaf node들 중 성능 개선에 가장 도움되는 leaf node를 찾아 그 node만 split -> 새로 생긴 leaf node 2개만 계산하면 되므로 엄청 빠름
- cf) level-wise tree growth : 이전 모델들은 차례로 모든 지점에서 split을 하므로 최대 리프노드 레벨 차=1(balanced tree)
- level-wise대비 장점
	- 동일한 max_depth일 때, 더 적은 수의 노드로 비슷한 성능을 냄
	- 메모리를 적게 차지하고 더 빠름
	- 데이터가 많아질수록 XGBoost보다 성능이 좋음(tree pruning한 효과가 나타날 것이라 짐작)
- 단점
	- 파라미터로 제한을 두지 않거나 $N\leq 10000$이면 overfitting이 되기 쉬움 ex)max_depth를 제한하지 않으면 한 쪽 트리로만 overfitting될 수 있음 -> 애초에 제한을 건다는 것을 가정하고 만든 모
	- 파라미터에 따라 예민하게 반응함
- Hyper-parameter Tuning : $30000\leq N$ 권장(코랩 기준. 코랩에서는 Update에 따라 LGBM이 GPU를 못 쓸 때가 많다)
	- CART 관련
		- max_depth : (**15~25**)
		- num_leaves : max(#leaf nodes) (**2^(max_depth) 이하. \[32,64,...]**)
		- min_child_samples : min_samples_leaf (**1~100**) -> 데이터 개수에 따라 num_leaves와 함께 생각
		- colsample_bytree : node split 시 동일한 세팅이어야 하기에 global randomization 제공
		- reg_lambda
	- Tree ensemble 관련
		- 
## CatBoost(2017)
