# 데이터는 벡터다
데이터를 수학적으로 설명하는 이유는 수학의 설득력 때문
하나의 feature가 분석 대상의 특성을 모두 나타내주지 못하기 때문에 다양한 관점에서 설명할 필요가 있다. → 이를 설명할 수 있는 게 벡터(집합은 연산이 불가능)
- **vector**
    ![](https://i.imgur.com/GtSvNw2.png)
    - vector의 의미
        - 물리학적 관점 : 길이와 방향이 주어진 화살표. 화살표를 이동시켜도 똑같다.
        - 컴퓨터공학적 관점 : 숫자의 리스트. 화살표가 항상 원점에서 시작한다.
        - 데이터사이언스적 관점 : 이 둘의 차용
    - 기본 연산 : 합, 스칼라곱
        - 합 : 화살표를 따라 차례로 이동한 최종 위치=각 차원의 원소 끼리 합
        - scaling : 벡터의 크기를 변화시키는 것=각 원소에 스칼라를 곱해 줌
        - scalar : 앞에 곱해져서 크기를 바꿔주는 숫자
    - 벡터 공간
        - 기저 벡터 : 임의의 벡터를 표현할 수 있는 기준이 되는 벡터. 선형 독립인 벡터의 집합.
        - span : 기저 벡터의 모든 선형 결합(스칼라 연산과 합)의 집합. (모든 벡터는 기저 벡터의 선형결합으로 만들어낼 수 있다.)
        - 스팬하는 경우 하나의 벡터를 화살표가 아닌, 점으로 생각하는 게 편함
        - 기저벡터가 될 수 있는 경우 : 선형 종속이 아닐 때
        - 선형 종속과 선형 독립
            - 선형 종속 : span의 축소 없이 하나 이상의 벡터를 제외시켜도 되는 경우
            - 선형 독립 : 각 벡터가 기존 스팬에 다른 차원을 추가해줄 수 있는 경우
### 데이터 사이언스에서 벡터의 정의
- 벡터 : list of numbers (computer science의 의미를 기본적으로 차용. 수치계산으로 모든 것을 다룰 수 있기 때문?). 물리적 의미도 필요할 때 가져와서 쓰면 됨
- 숫자 묶음 하나=하나의 개념
    - data profiling : 데이터를 수치로 정의하는 것 = 벡터 공간의 정의
        
        cf) user profiling : 고객 데이터를 수치로 표현하는 것
        
    - 축 : 벡터의 각 원소의 의미(x1,x2,x3,…)
    - 차원 : 벡터 원소 개수(n개) $v \in \mathbb{R}^n$﻿ (선형성 : $\mathbb{R}^n \rightarrow \mathbb{R}^n$﻿ )
    - 벡터 공간 : 의미를 가지는 숫자 묶음의 모임. 벡터의 집합
    - 벡터의 크기(norm) = 벡터의 길이
    - 벡터의 방향 = 데이터의 경향성. 단위 벡터의 스팬으로 만들어짐
- basis(기준,축) 변화에 따른 수치의 의미 변화 : 기준이 무엇이냐에 따라 데이터 해석이 완전히 달라진다. 꼭 기준이 column일 필요는 없고, 특정 데이터일 수도 있음 ⇒ 왜 기준이 그렇게 정의된 건지가 중요. 얼마나 제대로 데이터를 수치화하는지가 중요!
- Feature scaling의 필요성 : 각 축의 범위가 너무 많이 차이가 나면 분포를 제대로 볼 수 없다. 각 컬럼마다 분포를 비슷하게 나타내면 치우치지 않음 ex)min_max scaling, standard scaling(정규분포)
### 데이터 분석을 위한 벡터 연산
- 같은 차원의 벡터끼리만 연산 가능(shape이 같은array)
- 벡터의 크기(L2 Norm) : $|x|=||x||_2=\sqrt{x_1^2+x_2^2+...}$﻿
    - ==L1 Norm==
        $||x||_1 =|x_1|+|x_2|+...$﻿
- 벡터 덧셈 : $x+y=(x_1+y_1,x_2+y_2,...)$﻿
- 스칼라배(scalar multiplication) : $ax=(ax_1,ax_2,...,ax_N)$﻿
- 벡터의 내적(inner product) : $x\cdot y=x_1y_1+x_2y_2+...+x_ny_n=|x||y|cos\theta$﻿ (각 벡터를 다른 벡터에 정사영시킨 것)
    - ==일(W)의 정의==
        힘과 힘의 방향으로 이동한 거리의 곱=이동 방향으로 가해진 힘의 크기와 이동 거리의 곱
        W=$\vec{F}\cdot \vec{x}=|F||x|cos\theta$﻿
    - ==me)데이터 간에 이건 무슨 의미일까?==
        내적은 두 벡터의 방향의 유사성을 볼 수 있는 스칼라값 → 두 데이터의 유사성을 볼 수 있음
        
- 데이터 간 유사성 : 벡터 간 거리 측정으로 판별. 데이터마다 어떤 것이 유의미하냐에 따라 거리를 다르게 정의해서 사용.
    ![](https://i.imgur.com/vnzZZe3.png)
    - Manhattan Distance(L1 distance) : $\sum ^N_i|x_i-y_i|$﻿ 실제이동거리
    - Euclidean Distance(L2 distance) : $\sqrt{\sum ^N_i(x_i-y_i)^2}$﻿ 직선 거리 (실제 이동거리(road network)와는 다름)
    - Cosine Distance : $1-cos\theta=1-{x\cdot y\over|x||y|}$﻿ (숫자가 작을수록 가까움)
        - cosine similarity ${x\cdot y\over|x||y|}$﻿ (숫자가 클수록 방향이 유사함)
        - ==me)그러면 코사인디스턴스는 반대 방향 데이터와의 차이는 모르겠네==
# Feature Space
- Feature Engineering : feature vector를 만드는 과정(머신러닝에서 가장 중요)
	![](https://i.imgur.com/RcbmYIt.png)
- Feature Vector : input vector에서 필요한 특징만을 추출하여 벡터로 표현한 것.
    - 특징 선별은 데이터를 잘 이해하고 있어야 가능(domain knowledge). 컴퓨터는 결국 계산기. 잘 쓰는 건 우리의 몫
        - ==Computational Thinking==
            ![](https://i.imgur.com/DoBGKIO.png)
            [http://www.icompute-uk.com/news/computational-thinking-2/](http://www.icompute-uk.com/news/computational-thinking-2/)
    - feature vector 만드는 법
        - feature selection : column의 의미를 기준으로 선택
        - feature extraction : 새로운 벡터를 생성하기
        - embedding model(BERT,GPT 등) 이용하기(딥러닝)
- Feature Space : feature vector로 span한 벡터 공간
    cf) 딥러닝에서는 Embedding Space
# Matrix곱은 vector space의 선형 변환이다

## determinant
- determinant는 단위 벡터 기준 공간에 대비해 넓이가 얼마나 변했는지를 말해주는 숫자이다.
	- determinant<0 : i와 j의 위치가 반대가 되었음을 의미한다.
	- determinant=0 : 차원을 축소시킴을 의미한다.
		- 해가 있는 경우 : 축소된 공간에 상수 벡터가 위치했을 때
		- 해가 없는 경우 : 축소된 공간 밖에 상수 벡터가 있을 때
- 영공간 : 행렬을 작용시킨 후 0벡터로 squeeze되는 벡터들의 공간
- 상수 벡터가 0일 때 : 해는 그 행렬의 0공간에 속해있는 것들이 된다. 
me) 데이터의 개수=행의 수인가?
# Dot Product
이중성 부분이 이해가 안 됨

[3Blue1Brown-Linear Algebra](https://www.3blue1brown.com/topics/linear-algebra)