# Classification의 정의
![[4.Classification  .excalidraw]]
- 여러 개의 **X**(vector)를 구분된 숫자의 모임에 속하는 원소 **y**(vector)로 대응시키는 관계 **f**(:classifier)를 찾는 것 : 컴퓨터가 데이터를 분류하는 방법
	- $y_i$ : 숫자 자체에 의미는 없고, 숫자가 다르다는 게 의미 있음
- 직관적 의미
	- 주어진 데이터(X)를 원하는 분류 기준(y)대로 나누는 방법
	- 주어진 데이터(X)에 원하는 분류 기준(y=label)을 붙이는 방법
	- 비슷한 특징을 가지는 데이터들을 같은 분류로 나누는 방법
# Model
## Linear Classifier
![[4.Classification  _0.excalidraw]]
- $f(X)=w_1x_1+w_2x_2+\cdots +w_0=0$ : y값을 알 수는 없음
- 분류 기준
	- x값과 함수 값 비교
		- $x_2-f(t)>0 : A$
		- $x_2-f(t)<0 : B$
		- $x_2=f(t)$ : 아무 데나 넣어주면 됨
	- 법선 벡터의 방향과의 일치성으로 판단
- Loss function : 함수 값과 x값 차이의 합
- SVM(Support Vector Machine)으로 발전. SVM도 Deep Learning이 나온 후 안 씀
	- 장점 : N<1000일 때 잘 됨
	- 단점 : 너무 느림, kernel[^1] trick에 따른 성능 편차 심함
	![kernel trick](https://i.imgur.com/QOy3B4E.png)[Link](https://www.researchgate.net/figure/Non-linear-classifier-using-Kernel-trick-16_fig4_340610860)
- 분류가 여러 개일 때(multi-calss classification) : One-vs-All
	- A vs ~A로 구분하여 분류
		- 어디에도 속하지 않는 것 : random하게 분류
		- 겹치는 부분에 있는 것 : 더 가까운 쪽으로 분류
	- non-linear model도 이런 원리로 분류
![](https://i.imgur.com/G3POQB6.png)[Image Link](https://cs231n.github.io/linear-classify/)
- 문제점과 대응책 : Most data sets are not linearly seperable => non-linear model+regularization
	- non-linear model : 외우는 능력이 있음 ex)Tree model(ML), Deep Learning
	- regularization : 모델에 제약을 걸어 overfitting을 방지하는 것
## Decision Tree - CART
- 어떤 기준에 따라 데이터를 분류하고 규칙들의 조합을 나무 구조로 나타내는 모델
- recursive partitioning : 구조가 부분에서 계속해서 반복되도록 만드는 것
- binary tree 사용
![[2.2.Classification  .excalidraw]]
- CART(Classification and Regression Trees)
	- 분류와 회귀가 모두 가능한 트리 -> 둘 중 하나를 선택해서 발전시킬 수 있다
	- 무조건 binary tree
	- non-parametric learning
		- train : tree build
		- predict : tree traverse(탐색)
	- 현존하는 모든 tree model의 base. Decision tree 중 유일하게 살아남았지만, 이것보다는 Random Forest를 씀
	1. Classification tree
		![](https://i.imgur.com/1JIi0rA.png)
		[Image Link](https://lucy-the-marketer.kr/ko/growth/decision-tree-and-impurity/)
		- Loss function : 불순도(impurity)가 낮아지는 방향 or 순도가 높아지는 방향으로 분류
			- Gini Index :=$I(A)=1-\displaystyle\sum^m_{k=1}(p_k)^2$
				- m : 클래스 개수
				- p : k를 뽑을 확률
				- $0\leq I(A)\leq 0.5$ 
				- me)제곱해서 빼지 않으면 항상 0이 나옴
			- Entropy :=$E(A)=-\displaystyle \sum^m_{k=1}p_{ik}log_2(p_{ik})$
				- i : node index
				- p : k를 뽑을 확률. 특정 클래스인 데이터를 뽑을 확률.
				- $0\leq E(A)\leq 1$
			- BCE(Binary Cross Entropy) :=$-\frac{1}{N}\displaystyle\sum^N_{i=0}y_ilog(\hat y_i)+(1-y_i)log(1-\hat y_i)$ me)이거 나중에 더 자세히 알아볼 것
		- 분류 원리 : Information gain이 가장 큰 경우부터 분리
			- Information gain : 분리 전 지니 계수-분리 후 지니 계수
			- 분리 후 Gini Index : $I(A)=\displaystyle \sum ^d_{i=1}R_i(1-\displaystyle\sum^m_{k=1}(p_{ik})^2)$
				- d : 분할 개수
				- R : 분할된 부분의 비율->me)개수가 많은 부분의 불순도가 더 중요하기 때문에 비율을 적용함
				- m : 클래스 개수
				- p : k를 뽑을 확률
			- [Numeric Feature인 경우 기준 산정 방법 3가지]([의사결정나무(Decision Tree) :: 독립변수가 연속형 일 때 (tistory.com)](https://leedakyeong.tistory.com/entry/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4Decision-Tree-%EB%8F%85%EB%A6%BD%EB%B3%80%EC%88%98%EA%B0%80-%EC%97%B0%EC%86%8D%ED%98%95-%EC%9D%BC-%EB%95%8C?category=843676))
				- simple approach : feature를 기준으로 정렬 후, 처음부터 끝까지, 데이터의 사잇값을 기준으로 Gini Index를 계산한다.(정렬은 nlogn이므로 시간이 좀 걸림) - Scikitlearn에서 사용하는 방법
				-  Heuristic approach : 목표 값이 바뀌는 point에서만 나눠서 Gini Index를 계산한다.
				- Quartile approach : 25%, 50%, 75% 지점을 먼저 잘라서 비교한다.(한 쪽으로 개수가 몰리면 Gini Index는 당연히 높기 때문)
		- 해석력을 가지기 위해서는 depth가 깊지 않은 게 좋다.
	2. Regression tree
		- x가 바뀜에 따라 대응되는 y가 있을 때 그 관계 f를 구하는 것
		- Regression이므로 y의 값이 중요
			- $y_i$ : 해당 영역에 속하는 y의 평균
		![[2.2.Classification  _0.excalidraw]]
		-  원리 : 분할 후 각 부분의 y값의 분산이 낮아지는 게 목표
			- SSR:=$\sum_i (y_i-\bar y_i)^2$
				- $y_i-\bar y_i$ : residual. 실제 값과 평균의 차
			- 분할 후 SSR : $\displaystyle \sum^d_{k=1} R_k\displaystyle \sum_{i=1}^n (y_i-\bar y_i)^2$
				- d : 분할 개수
				- n : 각 부분에 속한 데이터 개수
				- R : 분할된 부분의 비율
- Regularization의 필요성
	- recursive partitioning이 종료되는 조건 : Gini index=0(분류), SSR=0(회귀) => 현실적으로 불가능하거나 overfitting되는 것 => $P_{train}$을 제한하고 $P_{test}$가 올라가도록 함
	- 제한 방법 3가지
		- Max Depth 설정
		- min-samples-leaf : leaf node에 들어가는 최소 데이터 개수 제한
		- min-samples-split : 데이터가 N개 이상일 때만 쪼개도록 제한
- 모델 평가 및 지표 해석 
	- 분류 대상의 개수가 편향적일 때 : Accuracy만으로는 분류의 정확도를 알 수 없음 
	- me)보충 CH02-03. Model 평가 및 지표 해석
## Random Forest(Ensemble)
- Ensemble : 예측 성능이 좋지 않은 모델들을 모아 예측 성능이 높은 모델을 만드는 방법
	```mermaid
	flowchart LR
		weak[weak learner]
		strong[strong learner]
		weak-->|ensemble|strong
	```
- Random Forest : 여러 개의 CART의 결과를 취합하여 결과를 내는 모델
- 원리 : Bagging -> Random subspace method
	
	![](https://i.imgur.com/auKqSBM.png)[Image Link]([Bootstrap aggregating - Wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating))
	- Bagging : 비슷하지만 조금씩 다른 데이터 모임에서 나온 결과를 통계적으로 일반화하는 방법
		- Bootstrapping : 원본 데이터 N개 -> 복원 추출 N개
			- 주어진 클래스의 비율을 확률로 하는 다항분포를 따르는 데이터 샘플링
			- 전체 데이터를 똑같이 여러 번 돌리면 동일한 결과가 나옴
	- Random subspace method[^2]
		- column space에서 일부 feature만 랜덤으로 뽑아 subspace를 만들어 학습을 시키는 방법
		- feature를 뽑는 두 가지 방법
			- Global randomization : 처음 한 번만 feature selection 후 분기마다 그 feature만 사용
			- Local randomization : 매 노드마다 새롭게 feature selection. feature를 버리는 것은 아니고, 모든 feature를 끝까지 유지함 -> Random Forest에서 쓰는 방식
			![[2.2.Classification  _1.excalidraw]]
		- Local randomization 방식의 장점 : 모든 feature에 대해 계산하지 않으므로 계산 속도가 빠름. greedy approach 예방.
		- Random Forest의 단점 : N>1000일 때 힘듦
		- skLearn API
			- CART 관련 parameter
				- max_depth : tree의 최대 깊이 (3~10 권장)
				- max_features : feature sampling 비율 (0.5~0.8 권장)
				- criterion : node split의 기준. 기본값 gini indes (회귀 : squared error, absolute error 사용)
				- min_samples_split[^3] : node split을 할 수 있는 최소 샘플 수 (2(default)~50 권장)
				- min_samples_leaf : leaf node가 가져야 하는 최소 샘플 수 (1(default)~20 권장)
			- Tree ensemble 관련 parameter
				- n_estimators : 앙상블하는 트리 수
				- random_state : 결과를 재현하기 위해 랜덤 함수를 돌릴 때 숫자 지정
				- n_jobs : 계산하는 물리 코어의 수(os.cpu_counts-1[^process] : 거의 최대로 끌어 쓰는 것)
		
[^1]: kernel : the part of a nut or seed inside the shell, or the part inside the stone of some fruits
[^2]: Random subspace method는 차원의 저주 문제 해결을 위해 제안된 기법이다.
[^3]: min_samples_split : leaf node의 최대 개수에 관여한다. max_depth가 너무 작으면 이 기준이 덜 적용되었을 때 끝날 수 있으므로, 이 기준을 우선 시 하여 max_depth를 조정한다. 특히 회귀의 경우 이 기준을 우선 적용한다. 분류는 max_depth를 우선하는 편.
[^process]: 파이썬은 thread를 쓸 수 없다.
	- process : physical core 단위 / thread : task 단위
	- sklearn : GPU를 못 써서 데이터가 많아지면 잘 안 돌아간다. GPU 안에는 수많은 단순 계산 파트가 들어있지만, CPU는 고급 기능이 같이 들어있는 대신 단순 계산 파트는 일부라서 데이터가 많으면 느리다.
	- GPU를 쓰는 라이브러리 : CuML, XGBoost, LightGBM, CatBoost