# Clustering의 정의
- 다른 그룹에 있는 것들보다 같은 그룹 내에 있는 데이터끼리 더 비슷하게 되도록 묶는 방법
- 직관적 의미 : 비슷한 데이터끼리 묶는 방법. 비슷하지 않은 데이터들은 다른 그룹으로 나누는 방법
- **unsupervised** classification(label이 없음)
	-> similarity measure+feature vector(data representation)에만 영향 받음
	=> 데이터에 맞는 similarity measure를 선택하는 게 중요
- 적용 예) 뉴스 클러스터링, 검색
# Model
## K-Means clustering(1950~) 
- 평균을 기준으로 데이터를 묶어줌
- 장점
	1. 엄청나게 빠른 속도
	2. 결과가 무조건 나옴(K-means는 최선의 결과를 항상 내놓는 모델)
	3. 이해가 쉬움(간단하고, 수학적 의미를 보기 좋음)
![[2.3.Clustering  .excalidraw]]
- 원리
	1. 랜덤으로 K개의 데이터 선택하여 기준점으로 삼기
	2. 이 데이터를 기준으로 다른 모든 데이터에 대해 가장 가까운 데이터 찾기(**반드시 euclidean distance)
	3. 가까운 데이터끼리 묶기(=같은 번호 부여)
	4. 각 클러스터의 평균을 구하고(K개) 새로운 기준점으로 삼기
	5. 2~4 반복
	6. 종료 기준
		- 업데이트 되는 데이터가 없음
		- Objective function[^1] 개선이 없음
		- (데이터가 많을 때)iteration[^2] 횟수 제한에 도달
- Objective function := $\displaystyle\sum^K_{j=1}\displaystyle\sum_{i\in C_j}L_2(x_i,c_j)$
	- $C_j$ : jth cluster
	- **$c_j$ : jth centroid** -> 이 식의 유일한 변수
	- O.F.=0 인 경우
		- 가장 이상적으로, 클러스터 내 모든 데이터가 특정 지점에 몰려있을 때
		- K=N이 되었을 때
	- elbow
		![[2.3.Clustering  _0.excalidraw]]
		- elbow 이 후부터 K를 계속 추가해도 Objective Function이 드라마틱하게 나아지지 않음
		- 문제점 : 비지도 학습이므로 elbow가 K의 진짜 최적점인지 장담할 수 없음
- K-means의 한계와 보완책
	- 초기에 선택되는 K개의 데이터에 따라 성능 편차 심함 -> K-means++(최대한 떨어진 데이터를 고르는 방법. 10번 정도 해보고 o.f.가 가장 낮은 걸 고름)
	- 평균이 의미 없는 데이터에서는 효과가 없음 -> K-modes(categorical data를 다루는 모델), K-prototype(mixed data를 다루는 모델)
	- outlier에 민감(outlier에 의해 평균이 계속 끌려감)-> K-mediods(중심값 사용. CLALANS로 발전)
	- 데이터 분포가 구형이 아니면[^3](ex.manifold) 클러스터 생성을 못함 -> HAC,DBSCAN
	- 클러스터링은 비지도 학습이기 때문에 결과가 잘 나왔는지 아닌지 명확히 알 수 없고, 결과가 잘 나와도 왜 그런지 설명할 수 없어서 애초에 지도 학습으로 하는 게 낫다.
## Hierarchical Agglomerative Clustering
- 데이터의 계층 구조를 파악하는 클러스터링 방법
- Agglomerative : 상향식. 작은 클러스터부터 시작해서 큰 클러스터를 만들어가는 방식. 
	- $N_c:N\rightarrow 1$
		- $N_c$ : 클러스터 개수
		- 전체 데이터만큼의 클러스터에서 1개가 될 때까지 데이터를 묶어준다
	- cf)Divisiable : 하향식. 데이터를 나누면서 하는 방법이므로 K-mean랑 비슷함. 그래서 안 씀. sklearn에 아예 없다.
	![](https://i.imgur.com/Vt5Iee8.png)
	![](https://i.imgur.com/cPEjCAH.png)[Image Link](https://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/#average-linkage)
- 원리
	1. 모든 데이터를 하나의 클러스터로 세팅
	2. 모든 데이터 중 가장 가까운 두 개의 데이터를 묶음
	3. 그 다음 가까운 데이터를 묶음
	4. 설정한 클러스터 개수가 되면 종료
- 사용법 : K=최종 클러스터 개수, metric=아무 거리 함수,linkage='single'
	- metric(측정)과 linkage를 어떻게 지정하느냐에 따라 전혀 다르게 작동
	- linkage : 뭐가 그 데이터에 좋을지는 써봐야 앎. 보통 average로 시작.([HAC linkages](https://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/#average-linkage))
		- single : 가장 가까운 pair가 포함된 클러스터 연결. (클러스터 간 거리가 가까울 때 chaining effect 때문에 super cluster가 생길 수 있음)
		- average : 두 클러스터 간 평균 거리가 가장 가까운 클러스터 연결
		- complete : 두 클러스터에서 가장 멀리 떨어진 pair 간의 거리가 가장 가까운 클러스터 연결
		- ward : 두 클러스터를 합쳤을 때, innertia(클러스터 내 분산)가 가장 작아지는 클러스터를 연결. ward+L2=K-means
		![](https://i.imgur.com/M1Uoy9g.png)
		[Image Link](https://www.analyticsvidhya.com/blog/2021/06/single-link-hierarchical-clustering-clearly-explained/)
- K-mean 대비 장점 : 성능적으로 우수
	1. initial point의 영향이 없음
	2. outlier에 의한 영향이 적음(마지막에 묶이기 때문)
	3. 모델 디자인에 따라 K-means의 문제를 모두 해결
		- linkage에 따라 여러 형태를 가진 데이터를 클러스터링 가능(데이터 간 연결성을 판단하기 때문)
		- categorical data : edit distance, hamming distance 사용하면 됨
- 치명적 단점
	1. 모든 데이터 사이의 거리를 계산하므로 $O(N^3)$이 걸림 (K-means는 $O(nki$))
	2. 예측의 기준이 없음



[^1]: 비지도 학습에서는 target이 없으므로 Loss function이라는 말 대신에 Objective function을 사용
[^2]: iteration : the repetition of a process or utterance
[^3]: 다변량 정규분포를 따르지 않는 것. 다변량 정규분포를 따른다고 가정하면 데이터를 조작해서 할 수 있는 게 많지만, 실제 데이터는 그럴 가능성이 아주 낮다. 그리고 손이 너무 많이 가면 머신러닝을 하는 의미가 퇴색된다. 